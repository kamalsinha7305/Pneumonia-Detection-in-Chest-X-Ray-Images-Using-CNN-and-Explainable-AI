{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamalsinha7305/Pneumonia-Detection-in-Chest-X-Ray-Images-Using-CNN-and-Explainable-AI/blob/main/pnemonia_detection_in_check_xray_FINAL_CODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1: Imports and Setup\n",
        "# Standard imports from your original notebook\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# TensorFlow and Keras imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# New import for Transfer Learning\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Scikit-learn for evaluation\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# OpenCV for Grad-CAM visualization\n",
        "import cv2\n",
        "\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "print(\"TensorFlow Hub Version:\", hub.__version__)"
      ],
      "metadata": {
        "id": "PzXbyDsIM5F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1: Imports and Data Download (Corrected)\n",
        "\n",
        "import kagglehub\n",
        "# This command downloads the dataset and returns the path where it's stored.\n",
        "# This MUST be the first step to ensure the files exist.\n",
        "print(\"Downloading dataset from Kaggle Hub...\")\n",
        "dataset_path_str = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")\n",
        "\n",
        "# Now, create the Path object from the returned string\n",
        "BASE = Path(dataset_path_str)\n",
        "\n",
        "print(f\"Dataset downloaded to: {BASE}\")\n",
        "print(f\"Directory exists: {BASE.exists()}\")\n",
        "print(f\"Contents of BASE: {os.listdir(BASE)}\")\n",
        "\n",
        "\n",
        "# Part 2: Dataset Loading and Preparation (This should now work)\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH = 32\n",
        "\n",
        "# The actual data is in a subfolder named 'chest_xray'\n",
        "DATA_DIR = BASE / 'chest_xray'\n",
        "\n",
        "# Part 2: Dataset Loading and Preparation (with Augmentation in the Pipeline)\n",
        "\n",
        "# Define the data augmentation as a sequential model\n",
        "# This keeps it separate from the main model architecture\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "], name=\"data_augmentation\")\n",
        "\n",
        "\n",
        "def make_ds(folder, shuffle=True, augment=False):\n",
        "    \"\"\"\n",
        "    Loads a dataset, normalizes it, and optionally applies data augmentation.\n",
        "    \"\"\"\n",
        "    ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        folder,\n",
        "        image_size=IMG_SIZE,\n",
        "        batch_size=BATCH,\n",
        "        label_mode=\"binary\",\n",
        "        shuffle=shuffle\n",
        "    )\n",
        "    class_names = ds.class_names\n",
        "\n",
        "    # Normalize the images to [0, 1]\n",
        "    ds = ds.map(lambda x, y: (x / 255.0, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    # Apply augmentation ONLY to the training dataset\n",
        "    if augment:\n",
        "        ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    # Prefetch for performance\n",
        "    return ds.prefetch(tf.data.AUTOTUNE), class_names\n",
        "\n",
        "# Load the datasets using the new function\n",
        "train_ds, class_names = make_ds(DATA_DIR / \"train\", shuffle=True, augment=True)\n",
        "val_ds, _ = make_ds(DATA_DIR / \"val\", shuffle=False, augment=False)\n",
        "test_ds, _ = make_ds(DATA_DIR / \"test\", shuffle=False, augment=False)\n",
        "\n",
        "print(\"\\nSuccessfully loaded datasets with augmentation in the pipeline!\")\n",
        "print(\"Classes:\", class_names)"
      ],
      "metadata": {
        "id": "8FpAkeYFM7v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1 and 2 remain the same (Imports and Data Loading)\n",
        "# ... Ensure your data (train_ds, val_ds, test_ds) is loaded before running this ...\n",
        "\n",
        "# Import the necessary components from Keras applications\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "# Part 3: Advanced Model Building - FINAL FIX v3\n",
        "# Part 3: Advanced Model Building (Simplified and Corrected)\n",
        "# Part 3: Advanced Model Building (Simplified and Corrected)\n",
        "\n",
        "def build_stable_model(input_shape=(224, 224, 3)):\n",
        "    \"\"\"\n",
        "    Builds the model WITHOUT internal data augmentation layers.\n",
        "    \"\"\"\n",
        "    # 1. Load the base model\n",
        "    base_model = MobileNetV2(input_shape=input_shape,\n",
        "                             include_top=False,\n",
        "                             weights='imagenet')\n",
        "    base_model.trainable = False # Freeze the base\n",
        "\n",
        "    # 2. Build the new model using the Functional API\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # 3. The input is passed directly to the base model\n",
        "    x = base_model(inputs, training=False)\n",
        "\n",
        "    # 4. Add the classifier head\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # 5. Create and compile the final model\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build the new, stable model\n",
        "model = build_stable_model()\n",
        "model.summary()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zwjbt9Q-NAvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 4: Model Training\n",
        "# We use the same callbacks as before for robust training.\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True, monitor='val_accuracy'),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=2, verbose=1)\n",
        "]\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "L64ukdKBNFPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. ENHANCED PERFORMANCE EVALUATION ---\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
        "\n",
        "# --- Original Evaluation Code ---\n",
        "# Get true labels and predictions from the test set\n",
        "y_true = np.concatenate([y.numpy().ravel() for _, y in test_ds])\n",
        "y_prob = model.predict(test_ds).ravel() # Probabilities from the model\n",
        "y_pred = (y_prob >= 0.5).astype(int) # Convert probabilities to class labels (0 or 1)\n",
        "\n",
        "# Print Classification Report\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "im = ax.imshow(cm, cmap='Blues')\n",
        "ax.set_xticks([0, 1]); ax.set_yticks([0, 1])\n",
        "ax.set_xticklabels(class_names); ax.set_yticklabels(class_names)\n",
        "plt.xlabel('Predicted', fontsize=12)\n",
        "plt.ylabel('True', fontsize=12)\n",
        "plt.title('Confusion Matrix', fontsize=14)\n",
        "for (i, j), v in np.ndenumerate(cm):\n",
        "    ax.text(j, i, str(v), ha='center', va='center', color='white' if cm[i, j] > cm.max()/2 else 'black', fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# --- ADDITION 1: ROC CURVE AND AUC SCORE ---\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nArea Under Curve (AUC) Score: {roc_auc:.4f}\")\n",
        "\n",
        "# --- ADDITION 2: VISUALIZE MISCLASSIFIED IMAGES ---\n",
        "# We need an unprocessed dataset for clean visualization\n",
        "unprocessed_test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    os.path.join(DATA_DIR, \"test\"), image_size=IMG_SIZE, batch_size=BATCH, shuffle=False\n",
        ")\n",
        "\n",
        "# Find indices of misclassified images\n",
        "misclassified_indices = np.where(y_pred != y_true)[0]\n",
        "correct_indices = np.where(y_pred == y_true)[0]\n",
        "\n",
        "# Unbatch the dataset to access individual images\n",
        "all_images = np.concatenate([x for x, y in unprocessed_test_ds])\n",
        "all_labels = np.concatenate([y for x, y in unprocessed_test_ds])\n",
        "\n",
        "# Plot a few False Positives (Normal predicted as Pneumonia)\n",
        "print(\"\\n--- Examples of Misclassified Images ---\\n\")\n",
        "fp_indices = np.intersect1d(np.where(y_true == 0), np.where(y_pred == 1))\n",
        "print(\"False Positives (NORMAL predicted as PNEUMONIA):\")\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i, index in enumerate(fp_indices[:5]): # Show up to 5 examples\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(all_images[index].astype('uint8'))\n",
        "    plt.title(f\"Pred: PNEUMONIA\\nTrue: NORMAL\\nProb: {y_prob[index]:.2f}\")\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Plot a few False Negatives (Pneumonia predicted as Normal)\n",
        "fn_indices = np.intersect1d(np.where(y_true == 1), np.where(y_pred == 0))\n",
        "print(\"\\nFalse Negatives (PNEUMONIA predicted as NORMAL):\")\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i, index in enumerate(fn_indices[:5]): # Show up to 5 examples\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(all_images[index].astype('uint8'))\n",
        "    plt.title(f\"Pred: NORMAL\\nTrue: PNEUMONIA\\nProb: {y_prob[index]:.2f}\")\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hSOJyLyINKGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime"
      ],
      "metadata": {
        "id": "n4uOuLuiV7lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p91d7m6KKyJ"
      },
      "outputs": [],
      "source": [
        "# Part 6 (Alternative 1): LIME (Local Interpretable Model-agnostic Explanations)\n",
        "# Make sure these imports are at the TOP of your notebook\n",
        "import lime\n",
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras import applications # <-- THE FIX IS ADDING THIS LINE\n",
        "\n",
        "print(\"\\n--- Generating LIME Explainability Visualization ---\")\n",
        "\n",
        "# We need an unprocessed dataset for clean visualization\n",
        "unprocessed_test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    os.path.join(DATA_DIR, \"test\"),\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=1,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Function to make predictions in the format LIME expects\n",
        "def predict_for_lime(images):\n",
        "    # LIME provides images in a different format, so we preprocess them for VGG16\n",
        "    processed_images = applications.vgg16.preprocess_input(images)\n",
        "    # Get the model's prediction probability for the \"PNEUMONIA\" class\n",
        "    preds = model.predict(processed_images)\n",
        "    # We need to return probabilities for both classes: [P(Normal), P(Pneumonia)]\n",
        "    return np.hstack((1 - preds, preds))\n",
        "\n",
        "# Create a LIME explainer object\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "# Take one image from the test set to explain\n",
        "for img_batch, label_batch in unprocessed_test_ds.take(1):\n",
        "    original_img = img_batch[0].numpy().astype('uint8')\n",
        "    true_label_index = int(label_batch.numpy()[0])\n",
        "\n",
        "    print(\"Generating LIME explanation... (This may take a moment)\")\n",
        "\n",
        "    # Generate the explanation\n",
        "    explanation = explainer.explain_instance(\n",
        "        original_img,\n",
        "        predict_for_lime,\n",
        "        top_labels=2,\n",
        "        hide_color=0,\n",
        "        num_samples=1000\n",
        "    )\n",
        "\n",
        "    # Get the image and mask for the top predicted label\n",
        "    temp, mask = explanation.get_image_and_mask(\n",
        "        explanation.top_labels[0],\n",
        "        positive_only=True,\n",
        "        num_features=5,\n",
        "        hide_rest=True\n",
        "    )\n",
        "\n",
        "    # Get the model's prediction\n",
        "    pred_prob = model.predict(applications.vgg16.preprocess_input(img_batch))[0][0]\n",
        "    prediction = \"PNEUMONIA\" if pred_prob > 0.5 else \"NORMAL\"\n",
        "    true_label = class_names[true_label_index]\n",
        "\n",
        "    # Display the result\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    ax1 = plt.subplot(1, 2, 1)\n",
        "    ax1.imshow(original_img)\n",
        "    ax1.set_title(f\"Original Image\\nTrue Label: {true_label}\")\n",
        "    ax1.axis('off')\n",
        "\n",
        "    ax2 = plt.subplot(1, 2, 2)\n",
        "    ax2.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n",
        "    ax2.set_title(f\"LIME Explanation\\nPrediction: {prediction}\")\n",
        "    ax2.axis('off')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Part 7: In-Text Explanation Generator ---\n",
        "\n",
        "def generate_text_explanation(pred_prob, true_label_index, class_names):\n",
        "    \"\"\"\n",
        "    Generates a human-readable text report based on the model's prediction.\n",
        "    \"\"\"\n",
        "    prediction_index = 1 if pred_prob > 0.5 else 0\n",
        "    prediction_name = class_names[prediction_index]\n",
        "    true_label_name = class_names[true_label_index]\n",
        "\n",
        "    report = f\"--- AI Analysis Report ---\\n\"\n",
        "    report += f\"True Label: {true_label_name}\\n\\n\"\n",
        "\n",
        "    confidence_str = \"\"\n",
        "    evidence_str = \"\"\n",
        "\n",
        "    if prediction_name == 'PNEUMONIA':\n",
        "        confidence = pred_prob * 100\n",
        "        if confidence > 90:\n",
        "            confidence_str = f\"Finding: PNEUMONIA (High Confidence: {confidence:.1f}%)\"\n",
        "            evidence_str = \"Evidence: Strong indicators of pneumonia, such as significant lung opacities, were detected.\"\n",
        "        elif confidence > 70:\n",
        "            confidence_str = f\"Finding: PNEUMONIA (Moderate Confidence: {confidence:.1f}%)\"\n",
        "            evidence_str = \"Evidence: Features consistent with pneumonia were detected. Clinical correlation is advised.\"\n",
        "        else:\n",
        "            confidence_str = f\"Finding: PNEUMONIA (Low Confidence: {confidence:.1f}%)\"\n",
        "            evidence_str = \"Evidence: Some indicators suggestive of pneumonia were found, but the signs are not definitive.\"\n",
        "\n",
        "    else: # NORMAL\n",
        "        confidence = (1 - pred_prob) * 100\n",
        "        if confidence > 90:\n",
        "            confidence_str = f\"Finding: NORMAL (High Confidence: {confidence:.1f}%)\"\n",
        "            evidence_str = \"Evidence: No significant indicators of pneumonia were detected in the visible lung fields.\"\n",
        "        else:\n",
        "            confidence_str = f\"Finding: NORMAL (Low Confidence: {confidence:.1f}%)\"\n",
        "            evidence_str = \"Evidence: The scan appears largely normal, but some minor ambiguities prevent a high-confidence assessment.\"\n",
        "\n",
        "    report += f\"{confidence_str}\\n\"\n",
        "    report += f\"{evidence_str}\\n\"\n",
        "    report += \"Recommendation: For clinical use, this analysis requires review by a qualified radiologist.\\n\"\n",
        "\n",
        "    return report\n",
        "\n",
        "# --- Generate and Print Reports for a Few Test Images ---\n",
        "\n",
        "# Get the unprocessed images and their true labels again\n",
        "unprocessed_test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    os.path.join(DATA_DIR, \"test\"),\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=1,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(\"\\n--- Generating In-Text Explanations for Test Images ---\\n\")\n",
        "\n",
        "for img_batch, label_batch in unprocessed_test_ds.take(3): # Generate for 3 examples\n",
        "    # Preprocess the image to get a prediction\n",
        "    processed_img = applications.vgg16.preprocess_input(img_batch)\n",
        "    prediction_probability = model.predict(processed_img)[0][0]\n",
        "\n",
        "    # Generate the report\n",
        "    true_label_idx = int(label_batch.numpy()[0])\n",
        "    explanation_report = generate_text_explanation(prediction_probability, true_label_idx, class_names)\n",
        "\n",
        "    # Display the image and its report\n",
        "    plt.imshow(img_batch[0].numpy().astype('uint8'))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    print(explanation_report)\n",
        "    print(\"-\" * 30)\n"
      ],
      "metadata": {
        "id": "nVg8ln-zMBw1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}